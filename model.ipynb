{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNysQ6uxG62jKN0mN7s+7mX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahiKhan5360/Segmentation-using-Capsule-layers-and-CNN/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyO4pbxMOjcg"
      },
      "outputs": [],
      "source": [
        "#model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, BatchNormalization, UpSampling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.metrics import MeanIoU\n"
      ],
      "metadata": {
        "id": "JUd6VRMbnQuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Capsule Layer Class\n",
        "def create_capsule_layer():\n",
        "    class CapsuleLayer(tf.keras.layers.Layer):\n",
        "        def __init__(self, num_capsules, capsule_dim, num_routing=3, **kwargs):\n",
        "            super(CapsuleLayer, self).__init__(**kwargs)\n",
        "            self.num_capsules = num_capsules\n",
        "            self.capsule_dim = capsule_dim\n",
        "            self.num_routing = num_routing\n",
        "\n",
        "        def build(self, input_shape):\n",
        "            self.input_height, self.input_width, self.input_channels = input_shape[1:4]\n",
        "            self.W = self.add_weight(\n",
        "                shape=[self.input_channels, self.num_capsules * self.capsule_dim],\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True\n",
        "            )\n",
        "         def call(self, inputs):\n",
        "            batch_size, height, width, channels = inputs.shape\n",
        "            tf.debugging.assert_equal(height, self.input_height, message=\"Height mismatch\")\n",
        "            tf.debugging.assert_equal(width, self.input_width, message=\"Width mismatch\")\n",
        "            tf.debugging.assert_equal(channels, self.input_channels, message=\"Channels mismatch\")\n",
        "\n",
        "            inputs_reshaped = tf.nn.relu(inputs)  # Apply ReLU\n",
        "            # shape (batch_size, height, width, channels)\n",
        "            # Transform each spatial position to capsules\n",
        "            u_hat = tf.tensordot(inputs_reshaped, self.W, axes=[[3], [0]])\n",
        "            u_hat = tf.reshape(u_hat, (-1, height, width, self.num_capsules, self.capsule_dim))\n",
        "\n",
        "            # Initialize routing logits\n",
        "            b = tf.zeros(shape=(tf.shape(inputs)[0], height, width, self.num_capsules))\n",
        "\n",
        "            # Dynamic routing\n",
        "            for i in range(self.num_routing):\n",
        "                c = tf.nn.softmax(b, axis=3)  # Softmax over num_capsules\n",
        "                s = tf.reduce_sum(c[..., None] * u_hat, axis=3)  # Sum over num_capsules\n",
        "                v = self.squash(s)  # Squash to get capsule vectors\n",
        "                if i < self.num_routing - 1:\n",
        "                    b += tf.reduce_sum(u_hat * v[..., None, :], axis=4)\n",
        "\n",
        "            return v  # Shape: (batch_size, height, width, capsule_dim)\n",
        "        def squash(self, s, axis=-1):\n",
        "            s_squared_norm = tf.reduce_sum(tf.square(s), axis=axis, keepdims=True)\n",
        "            scale = s_squared_norm / (1 + s_squared_norm)\n",
        "            return scale * s / tf.sqrt(s_squared_norm + 1e-9)\n",
        "\n",
        "    return CapsuleLayer\n",
        "\n",
        "# Model Creation for Binary Image Segmentation\n",
        "def create_capsule_segmentation_model(input_shape=(256, 256, 3), num_capsules=8, capsule_dim=16):\n",
        "    CapsuleLayer = create_capsule_layer()\n",
        "\n",
        "    # Input layer\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Encoder: Convolutional block 1\n",
        "    x = Conv2D(filters=64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.0002))(input_layer)\n",
        "    x = Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    # Encoder: Convolutional block 2\n",
        "    x = Conv2D(filters=128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.0002))(x)\n",
        "    x = Conv2D(filters=128, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    # Capsule layer\n",
        "    x = CapsuleLayer(num_capsules=num_capsules, capsule_dim=capsule_dim)(x)\n",
        "\n",
        "    # Decoder: Upsampling and convolutional layers\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(filters=128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.0002))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(filters=64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.0002))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    # Output layer: Single channel with sigmoid for binary segmentation\n",
        "    output_layer = Conv2D(filters=1, kernel_size=1, activation='sigmoid', padding='same')(x)\n",
        "\n",
        "     # Create model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    # Compile model with additional IoU metric\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0005),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', MeanIoU(num_classes=2)]  # Binary segmentation: background + lesion\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "    # Create and compile model\n",
        "    model = create_capsule_segmentation_model(input_shape=(256, 256, 3))\n",
        "    model.summary()\n",
        "\n",
        "      # Define callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "        ModelCheckpoint(filepath='/content/drive/MyDrive/ISIC2018/best_capsule_model.keras', save_best_only=True, monitor='val_loss'),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7)\n",
        "    ]"
      ],
      "metadata": {
        "id": "-g3BQ4gxnc9x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}