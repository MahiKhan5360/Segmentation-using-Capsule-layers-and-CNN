{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3nJtAjANbC3+AW8pjDixY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahiKhan5360/Segmentation-using-Capsule-layers-and-CNN/blob/main/Dataset_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if running in Colab\n",
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    print(\"Running in Google Colab environment.\")\n",
        "else:\n",
        "    print(\"Not running in Google Colab environment.\")\n",
        "\n",
        "# Mount Google Drive if in Colab\n",
        "if is_colab():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "\n",
        "# Fix for keras.utils.generic_utils error\n",
        "import os\n",
        "os.environ['TF_KERAS'] = '1'  # Force using tf.keras instead of standalone keras\n",
        "\n",
        "# Install required packages\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Fix for TensorFlow Addons compatibility\n",
        "!pip uninstall -y tensorflow-addons\n",
        "!pip install tensorflow-addons==0.17.1\n",
        "\n",
        "# Monkey patch for missing generic_utils\n",
        "import keras\n",
        "if not hasattr(keras.utils, 'generic_utils'):\n",
        "    import tensorflow as tf\n",
        "    keras.utils.generic_utils = tf.keras.utils\n",
        "\n",
        "# Check if packages are already installed to avoid reinstallation\n",
        "required_packages = {\n",
        "    'tensorflow': 'tensorflow>=2.8.0',\n",
        "    'tensorflow_addons': 'tensorflow-addons==0.17.1',  # with specific version\n",
        "    'segmentation_models': 'git+https://github.com/qubvel/segmentation_models',  # Use GitHub version\n",
        "    'albumentations': 'albumentations>=1.1.0',\n",
        "    'opencv-python': 'opencv-python>=4.5.5',\n",
        "    'scikit-image': 'scikit-image>=0.19.2',\n",
        "    'matplotlib': 'matplotlib>=3.5.1',\n",
        "    'pandas': 'pandas>=1.4.2',\n",
        "    'tqdm': 'tqdm>=4.64.0',\n",
        "    'einops': 'einops>=0.4.1',\n",
        "    'timm': 'timm>=0.5.4',\n",
        "    'seaborn': 'seaborn>=0.11.2'\n",
        "}\n",
        "\n",
        "# Only install packages that are not already installed\n",
        "for package, install_name in required_packages.items():\n",
        "    if package == 'tensorflow_addons':  # Skip as we already installed it\n",
        "        continue\n",
        "    if package == 'segmentation_models':  # Special handling for segmentation_models\n",
        "        try:\n",
        "            __import__(package)\n",
        "            print(f\"{package} is already installed.\")\n",
        "        except ImportError:\n",
        "            print(f\"Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", install_name])\n",
        "            print(f\"{package} installed successfully.\")\n",
        "    else:\n",
        "        try:\n",
        "            __import__(package)\n",
        "            print(f\"{package} is already installed.\")\n",
        "        except ImportError:\n",
        "            print(f\"Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", install_name])\n",
        "            print(f\"{package} installed successfully.\")\n",
        "\n",
        "# Set up TensorFlow and GPU configurations\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check for GPU availability\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Set memory growth to avoid allocating all memory at once\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"Found {len(gpus)} GPU(s). Memory growth enabled.\")\n",
        "\n",
        "        # Print GPU information\n",
        "        gpu_info = !nvidia-smi\n",
        "        print(\"GPU Information:\")\n",
        "        for line in gpu_info:\n",
        "            print(line)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"GPU configuration error: {e}\")\n",
        "else:\n",
        "    print(\"No GPU found. Running on CPU.\")\n",
        "\n",
        "# Set up mixed precision for faster training\n",
        "try:\n",
        "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "    tf.keras.mixed_precision.set_global_policy(policy)\n",
        "    print(f\"Mixed precision policy set to: {policy.name}\")\n",
        "except:\n",
        "    print(\"Mixed precision not supported or failed to set up.\")\n",
        "\n",
        "# Print TensorFlow version\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Verify dataset paths\n",
        "import os\n",
        "\n",
        "# Original dataset paths\n",
        "original_input_path = '/content/drive/MyDrive/ISIC2018_original/ISIC2018_Task1_2_Training_Input'\n",
        "original_gt_path = '/content/drive/MyDrive/ISIC2018_original/ISIC2018_Task1_Training_GroundTruth'\n",
        "\n",
        "# Processed dataset paths\n",
        "processed_base_path = '/content/drive/MyDrive/ISIC2018'\n",
        "\n",
        "# Check if original dataset exists\n",
        "if os.path.exists(original_input_path) and os.path.exists(original_gt_path):\n",
        "    input_files = len(os.listdir(original_input_path))\n",
        "    gt_files = len(os.listdir(original_gt_path))\n",
        "    print(f\"Original dataset found:\")\n",
        "    print(f\"  - Input images: {input_files}\")\n",
        "    print(f\"  - Ground truth masks: {gt_files}\")\n",
        "else:\n",
        "    print(\"Warning: Original dataset not found at the specified path.\")\n",
        "    print(f\"Expected paths: \\n  {original_input_path}\\n  {original_gt_path}\")\n",
        "\n",
        "# Check if processed dataset directory exists\n",
        "if not os.path.exists(processed_base_path):\n",
        "    print(f\"Creating processed dataset directory at: {processed_base_path}\")\n",
        "    os.makedirs(processed_base_path, exist_ok=True)\n",
        "else:\n",
        "    print(f\"Processed dataset directory exists at: {processed_base_path}\")\n"
      ],
      "metadata": {
        "id": "fkjZXbsHN1Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import albumentations as A\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "from glob import glob\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define paths (these should match what you set in the setup_environment cell)\n",
        "original_input_path = '/content/drive/MyDrive/ISIC2018_original/ISIC2018_Task1_2_Training_Input'\n",
        "original_gt_path = '/content/drive/MyDrive/ISIC2018_original/ISIC2018_Task1_Training_GroundTruth'\n",
        "processed_base_path = '/content/drive/MyDrive/ISIC2018'\n",
        "\n",
        "class ISIC2018DataPreprocessor:\n",
        "    def __init__(self,\n",
        "                 original_input_path,\n",
        "                 original_gt_path,\n",
        "                 processed_base_path,\n",
        "                 img_size=(256, 256),\n",
        "                 val_split=0.15,\n",
        "                 test_split=0.15):\n",
        "        \"\"\"\n",
        "        Initialize the ISIC2018 data preprocessor.\n",
        "\n",
        "        Args:\n",
        "            original_input_path: Path to original input images\n",
        "            original_gt_path: Path to original ground truth masks\n",
        "            processed_base_path: Base path for processed dataset\n",
        "            img_size: Target image size (height, width)\n",
        "            val_split: Validation split ratio\n",
        "            test_split: Test split ratio\n",
        "        \"\"\"\n",
        "        self.original_input_path = original_input_path\n",
        "        self.original_gt_path = original_gt_path\n",
        "        self.processed_base_path = processed_base_path\n",
        "        self.img_size = img_size\n",
        "        self.val_split = val_split\n",
        "        self.test_split = test_split\n",
        "\n",
        "        # Create processed dataset directories\n",
        "        self.train_img_dir = os.path.join(processed_base_path, 'train', 'images')\n",
        "        self.train_mask_dir = os.path.join(processed_base_path, 'train', 'masks')\n",
        "        self.val_img_dir = os.path.join(processed_base_path, 'validation', 'images')\n",
        "        self.val_mask_dir = os.path.join(processed_base_path, 'validation', 'masks')\n",
        "        self.test_img_dir = os.path.join(processed_base_path, 'test', 'images')\n",
        "        self.test_mask_dir = os.path.join(processed_base_path, 'test', 'masks')\n",
        "\n",
        "        self._create_directories()"
      ],
      "metadata": {
        "id": "M6oxBUReOecS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}