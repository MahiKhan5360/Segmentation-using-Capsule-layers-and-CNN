{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpi/LTcZ0P/Tzs3eZRFXMN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahiKhan5360/Segmentation-using-Capsule-layers-and-CNN/blob/main/Dataset_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if running in Colab\n",
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    print(\"Running in Google Colab environment.\")\n",
        "else:\n",
        "    print(\"Not running in Google Colab environment.\")\n",
        "\n",
        "# Mount Google Drive if in Colab\n",
        "if is_colab():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "\n",
        "# Fix for keras.utils.generic_utils error\n",
        "import os\n",
        "os.environ['TF_KERAS'] = '1'  # Force using tf.keras instead of standalone keras\n",
        "\n",
        "# Install required packages\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Fix for TensorFlow Addons compatibility\n",
        "!pip uninstall -y tensorflow-addons\n",
        "!pip install tensorflow-addons==0.17.1\n",
        "\n",
        "# Monkey patch for missing generic_utils\n",
        "import keras\n",
        "if not hasattr(keras.utils, 'generic_utils'):\n",
        "    import tensorflow as tf\n",
        "    keras.utils.generic_utils = tf.keras.utils\n",
        "\n",
        "# Check if packages are already installed to avoid reinstallation\n",
        "required_packages = {\n",
        "    'tensorflow': 'tensorflow>=2.8.0',\n",
        "    'tensorflow_addons': 'tensorflow-addons==0.17.1',  # with specific version\n",
        "    'segmentation_models': 'git+https://github.com/qubvel/segmentation_models',  # Use GitHub version\n",
        "    'albumentations': 'albumentations>=1.1.0',\n",
        "    'opencv-python': 'opencv-python>=4.5.5',\n",
        "    'scikit-image': 'scikit-image>=0.19.2',\n",
        "    'matplotlib': 'matplotlib>=3.5.1',\n",
        "    'pandas': 'pandas>=1.4.2',\n",
        "    'tqdm': 'tqdm>=4.64.0',\n",
        "    'einops': 'einops>=0.4.1',\n",
        "    'timm': 'timm>=0.5.4',\n",
        "    'seaborn': 'seaborn>=0.11.2'\n",
        "}\n",
        "\n",
        "# Only install packages that are not already installed\n",
        "for package, install_name in required_packages.items():\n",
        "    if package == 'tensorflow_addons':  # Skip as we already installed it\n",
        "        continue\n",
        "    if package == 'segmentation_models':  # Special handling for segmentation_models\n",
        "        try:\n",
        "            __import__(package)\n",
        "            print(f\"{package} is already installed.\")\n",
        "        except ImportError:\n",
        "            print(f\"Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", install_name])\n",
        "            print(f\"{package} installed successfully.\")\n",
        "    else:\n",
        "        try:\n",
        "            __import__(package)\n",
        "            print(f\"{package} is already installed.\")\n",
        "        except ImportError:\n",
        "            print(f\"Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", install_name])\n",
        "            print(f\"{package} installed successfully.\")\n",
        "\n",
        "# Set up TensorFlow and GPU configurations\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check for GPU availability\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Set memory growth to avoid allocating all memory at once\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"Found {len(gpus)} GPU(s). Memory growth enabled.\")\n",
        "\n",
        "        # Print GPU information\n",
        "        gpu_info = !nvidia-smi\n",
        "        print(\"GPU Information:\")\n",
        "        for line in gpu_info:\n",
        "            print(line)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"GPU configuration error: {e}\")\n",
        "else:\n",
        "    print(\"No GPU found. Running on CPU.\")\n",
        "\n",
        "# Set up mixed precision for faster training\n",
        "try:\n",
        "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "    tf.keras.mixed_precision.set_global_policy(policy)\n",
        "    print(f\"Mixed precision policy set to: {policy.name}\")\n",
        "except:\n",
        "    print(\"Mixed precision not supported or failed to set up.\")\n",
        "\n",
        "# Print TensorFlow version\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Verify dataset paths\n",
        "import os\n",
        "\n",
        "# Original dataset paths\n",
        "original_input_path = '/content/drive/MyDrive/ISIC2018_original/ISIC2018_Task1_2_Training_Input'\n",
        "original_gt_path = '/content/drive/MyDrive/ISIC2018_original/ISIC2018_Task1_Training_GroundTruth'\n",
        "\n",
        "# Processed dataset paths\n",
        "processed_base_path = '/content/drive/MyDrive/ISIC2018'\n",
        "\n",
        "# Check if original dataset exists\n",
        "if os.path.exists(original_input_path) and os.path.exists(original_gt_path):\n",
        "    input_files = len(os.listdir(original_input_path))\n",
        "    gt_files = len(os.listdir(original_gt_path))\n",
        "    print(f\"Original dataset found:\")\n",
        "    print(f\"  - Input images: {input_files}\")\n",
        "    print(f\"  - Ground truth masks: {gt_files}\")\n",
        "else:\n",
        "    print(\"Warning: Original dataset not found at the specified path.\")\n",
        "    print(f\"Expected paths: \\n  {original_input_path}\\n  {original_gt_path}\")\n",
        "\n",
        "# Check if processed dataset directory exists\n",
        "if not os.path.exists(processed_base_path):\n",
        "    print(f\"Creating processed dataset directory at: {processed_base_path}\")\n",
        "    os.makedirs(processed_base_path, exist_ok=True)\n",
        "else:\n",
        "    print(f\"Processed dataset directory exists at: {processed_base_path}\")\n"
      ],
      "metadata": {
        "id": "fkjZXbsHN1Cz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}