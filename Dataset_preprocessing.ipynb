{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahiKhan5360/Segmentation-using-Capsule-layers-and-CNN/blob/main/Dataset_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if running in Colab\n",
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    print(\"Running in Google Colab environment.\")\n",
        "else:\n",
        "    print(\"Not running in Google Colab environment.\")\n",
        "\n",
        "# Mount Google Drive if in Colab\n",
        "if is_colab():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "\n",
        "# Fix for keras.utils.generic_utils error\n",
        "import os\n",
        "os.environ['TF_KERAS'] = '1'  # Force using tf.keras instead of standalone keras\n",
        "\n",
        "# Install required packages\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Fix for TensorFlow Addons compatibility\n",
        "!pip uninstall -y tensorflow-addons\n",
        "!pip install tensorflow-addons==0.17.1\n",
        "\n",
        "# Monkey patch for missing generic_utils\n",
        "import keras\n",
        "if not hasattr(keras.utils, 'generic_utils'):\n",
        "    import tensorflow as tf\n",
        "    keras.utils.generic_utils = tf.keras.utils\n",
        "\n",
        "# Check if packages are already installed to avoid reinstallation\n",
        "required_packages = {\n",
        "    'tensorflow': 'tensorflow>=2.8.0',\n",
        "    'tensorflow_addons': 'tensorflow-addons==0.17.1',  # with specific version\n",
        "    'segmentation_models': 'git+https://github.com/qubvel/segmentation_models',  # Use GitHub version\n",
        "    'albumentations': 'albumentations>=1.1.0',\n",
        "    'opencv-python': 'opencv-python>=4.5.5',\n",
        "    'scikit-image': 'scikit-image>=0.19.2',\n",
        "    'matplotlib': 'matplotlib>=3.5.1',\n",
        "    'pandas': 'pandas>=1.4.2',\n",
        "    'tqdm': 'tqdm>=4.64.0',\n",
        "    'einops': 'einops>=0.4.1',\n",
        "    'timm': 'timm>=0.5.4',\n",
        "    'seaborn': 'seaborn>=0.11.2'\n",
        "}\n",
        "\n",
        "# Only install packages that are not already installed\n",
        "for package, install_name in required_packages.items():\n",
        "    if package == 'tensorflow_addons':  # Skip as we already installed it\n",
        "        continue\n",
        "    if package == 'segmentation_models':  # Special handling for segmentation_models\n",
        "        try:\n",
        "            __import__(package)\n",
        "            print(f\"{package} is already installed.\")\n",
        "        except ImportError:\n",
        "            print(f\"Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", install_name])\n",
        "            print(f\"{package} installed successfully.\")\n",
        "    else:\n",
        "        try:\n",
        "            __import__(package)\n",
        "            print(f\"{package} is already installed.\")\n",
        "        except ImportError:\n",
        "            print(f\"Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", install_name])\n",
        "            print(f\"{package} installed successfully.\")\n",
        "\n",
        "# Set up TensorFlow and GPU configurations\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check for GPU availability\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Set memory growth to avoid allocating all memory at once\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"Found {len(gpus)} GPU(s). Memory growth enabled.\")\n",
        "\n",
        "        # Print GPU information\n",
        "        gpu_info = !nvidia-smi\n",
        "        print(\"GPU Information:\")\n",
        "        for line in gpu_info:\n",
        "            print(line)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"GPU configuration error: {e}\")\n",
        "else:\n",
        "    print(\"No GPU found. Running on CPU.\")\n",
        "\n",
        "# Set up mixed precision for faster training\n",
        "try:\n",
        "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "    tf.keras.mixed_precision.set_global_policy(policy)\n",
        "    print(f\"Mixed precision policy set to: {policy.name}\")\n",
        "except:\n",
        "    print(\"Mixed precision not supported or failed to set up.\")\n",
        "\n",
        "# Print TensorFlow version\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Verify dataset paths\n",
        "import os\n",
        "\n",
        "# Original dataset paths\n",
        "original_input_path = '/content/drive/MyDrive/ISIC2018_original/ISIC2018_Task1_2_Training_Input'\n",
        "original_gt_path = '/content/drive/MyDrive/ISIC2018_original/ISIC2018_Task1_Training_GroundTruth'\n",
        "\n",
        "# Processed dataset paths\n",
        "processed_base_path = '/content/drive/MyDrive/ISIC2018'\n",
        "\n",
        "# Check if original dataset exists\n",
        "if os.path.exists(original_input_path) and os.path.exists(original_gt_path):\n",
        "    input_files = len(os.listdir(original_input_path))\n",
        "    gt_files = len(os.listdir(original_gt_path))\n",
        "    print(f\"Original dataset found:\")\n",
        "    print(f\"  - Input images: {input_files}\")\n",
        "    print(f\"  - Ground truth masks: {gt_files}\")\n",
        "else:\n",
        "    print(\"Warning: Original dataset not found at the specified path.\")\n",
        "    print(f\"Expected paths: \\n  {original_input_path}\\n  {original_gt_path}\")\n",
        "\n",
        "# Check if processed dataset directory exists\n",
        "if not os.path.exists(processed_base_path):\n",
        "    print(f\"Creating processed dataset directory at: {processed_base_path}\")\n",
        "    os.makedirs(processed_base_path, exist_ok=True)\n",
        "else:\n",
        "    print(f\"Processed dataset directory exists at: {processed_base_path}\")\n"
      ],
      "metadata": {
        "id": "fkjZXbsHN1Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import albumentations as A\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "from glob import glob\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define paths (these should match what you set in the setup_environment cell)\n",
        "original_input_path = '/content/drive/MyDrive/ISIC2018_original/ISIC2018_Task1_2_Training_Input'\n",
        "original_gt_path = '/content/drive/MyDrive/ISIC2018_original/ISIC2018_Task1_Training_GroundTruth'\n",
        "processed_base_path = '/content/drive/MyDrive/ISIC2018'\n",
        "\n",
        "class ISIC2018DataPreprocessor:\n",
        "    def __init__(self,\n",
        "                 original_input_path,\n",
        "                 original_gt_path,\n",
        "                 processed_base_path,\n",
        "                 img_size=(256, 256),\n",
        "                 val_split=0.15,\n",
        "                 test_split=0.15):\n",
        "        \"\"\"\n",
        "        Initialize the ISIC2018 data preprocessor.\n",
        "\n",
        "        Args:\n",
        "            original_input_path: Path to original input images\n",
        "            original_gt_path: Path to original ground truth masks\n",
        "            processed_base_path: Base path for processed dataset\n",
        "            img_size: Target image size (height, width)\n",
        "            val_split: Validation split ratio\n",
        "            test_split: Test split ratio\n",
        "        \"\"\"\n",
        "        self.original_input_path = original_input_path\n",
        "        self.original_gt_path = original_gt_path\n",
        "        self.processed_base_path = processed_base_path\n",
        "        self.img_size = img_size\n",
        "        self.val_split = val_split\n",
        "        self.test_split = test_split\n",
        "\n",
        "        # Create processed dataset directories\n",
        "        self.train_img_dir = os.path.join(processed_base_path, 'train', 'images')\n",
        "        self.train_mask_dir = os.path.join(processed_base_path, 'train', 'masks')\n",
        "        self.val_img_dir = os.path.join(processed_base_path, 'validation', 'images')\n",
        "        self.val_mask_dir = os.path.join(processed_base_path, 'validation', 'masks')\n",
        "        self.test_img_dir = os.path.join(processed_base_path, 'test', 'images')\n",
        "        self.test_mask_dir = os.path.join(processed_base_path, 'test', 'masks')\n",
        "\n",
        "        self._create_directories()\n",
        "\n",
        "     def _create_directories(self):\n",
        "        \"\"\"Create necessary directories for processed dataset.\"\"\"\n",
        "        directories = [\n",
        "            self.train_img_dir, self.train_mask_dir,\n",
        "            self.val_img_dir, self.val_mask_dir,\n",
        "            self.test_img_dir, self.test_mask_dir\n",
        "        ]\n",
        "\n",
        "        for directory in directories:\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "        print(\"Created processed dataset directories.\")\n",
        "\n",
        "\n",
        "    def organize_dataset(self):\n",
        "        \"\"\"\n",
        "        Organize the dataset by matching input images with their corresponding masks\n",
        "        and splitting into train, validation, and test sets.\n",
        "        \"\"\"\n",
        "        print(\"Organizing dataset...\")\n",
        "\n",
        "        # Get list of all input images and masks\n",
        "        input_files = sorted(os.listdir(self.original_input_path))\n",
        "        mask_files = sorted(os.listdir(self.original_gt_path))\n",
        "\n",
        "        # Extract image IDs from filenames\n",
        "        input_ids = [os.path.splitext(f)[0] for f in input_files]\n",
        "        mask_ids = [os.path.splitext(f)[0].replace('_segmentation', '') for f in mask_files]\n",
        "\n",
        "        # Create mapping from image ID to filenames\n",
        "        input_map = {id: f for id, f in zip(input_ids, input_files)}\n",
        "        mask_map = {id: f for id, f in zip(mask_ids, mask_files)}\n",
        "\n",
        "        # Find common IDs (images that have corresponding masks)\n",
        "        common_ids = sorted(list(set(input_ids).intersection(set(mask_ids))))\n",
        "\n",
        "        print(f\"Found {len(common_ids)} images with corresponding masks.\")\n",
        "\n",
        "        # Split into train, validation, and test sets\n",
        "        train_ids, temp_ids = train_test_split(\n",
        "            common_ids,\n",
        "            test_size=self.val_split + self.test_split,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Further split temp_ids into validation and test\n",
        "        val_ratio = self.val_split / (self.val_split + self.test_split)\n",
        "        val_ids, test_ids = train_test_split(\n",
        "            temp_ids,\n",
        "            test_size=1 - val_ratio,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        print(f\"Split dataset: {len(train_ids)} train, {len(val_ids)} validation, {len(test_ids)} test\")\n",
        "\n",
        "        # Copy files to their respective directories\n",
        "        self._copy_files(train_ids, input_map, mask_map, self.train_img_dir, self.train_mask_dir)\n",
        "        self._copy_files(val_ids, input_map, mask_map, self.val_img_dir, self.val_mask_dir)\n",
        "        self._copy_files(test_ids, input_map, mask_map, self.test_img_dir, self.test_mask_dir)\n",
        "\n",
        "        return {\n",
        "            'train_ids': train_ids,\n",
        "            'val_ids': val_ids,\n",
        "            'test_ids': test_ids\n",
        "        }\n",
        "\n",
        "\n",
        "    def _copy_files(self, ids, input_map, mask_map, img_dir, mask_dir):\n",
        "        \"\"\"\n",
        "        Copy image and mask files to their respective directories.\n",
        "\n",
        "        Args:\n",
        "            ids: List of image IDs to copy\n",
        "            input_map: Mapping from image ID to input filename\n",
        "            mask_map: Mapping from image ID to mask filename\n",
        "            img_dir: Destination directory for images\n",
        "            mask_dir: Destination directory for masks\n",
        "        \"\"\"\n",
        "        for id in tqdm(ids, desc=f\"Copying files to {os.path.basename(os.path.dirname(img_dir))}\"):\n",
        "            # Copy input image\n",
        "            src_img = os.path.join(self.original_input_path, input_map[id])\n",
        "            dst_img = os.path.join(img_dir, f\"{id}.jpg\")\n",
        "\n",
        "            # Copy mask\n",
        "            src_mask = os.path.join(self.original_gt_path, mask_map[id])\n",
        "            dst_mask = os.path.join(mask_dir, f\"{id}_mask.png\")\n",
        "\n",
        "            # Read, resize, and save image\n",
        "            img = cv2.imread(src_img)\n",
        "            img = cv2.resize(img, self.img_size[::-1])  # cv2 uses (width, height)\n",
        "            cv2.imwrite(dst_img, img)\n",
        "\n",
        "            # Read, resize, and save mask\n",
        "            mask = cv2.imread(src_mask, cv2.IMREAD_GRAYSCALE)\n",
        "            mask = cv2.resize(mask, self.img_size[::-1], interpolation=cv2.INTER_NEAREST)\n",
        "            # Ensure binary mask (0 or 255)\n",
        "            mask = (mask > 127).astype(np.uint8) * 255\n",
        "            cv2.imwrite(dst_mask, mask)\n",
        "\n",
        "    def create_augmentation_pipeline(self):\n",
        "        \"\"\"\n",
        "        Create data augmentation pipelines for training and validation.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing train and validation augmentation pipelines\n",
        "        \"\"\"\n",
        "        # Training augmentations\n",
        "        train_augmentation = A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            A.RandomRotate90(p=0.5),\n",
        "            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "            A.OneOf([\n",
        "                A.ElasticTransform(alpha=120, sigma=120 * 0.05, p=0.5),  # Remove alpha_affine\n",
        "                A.GridDistortion(p=0.5),\n",
        "                A.OpticalDistortion(distort_limit=1.0, p=0.5),  # Remove shift_limit\n",
        "            ], p=0.3),\n",
        "\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ])\n",
        "\n",
        "        # Validation/Test augmentations (only normalization)\n",
        "        val_augmentation = A.Compose([\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ])\n",
        "\n",
        "        return {\n",
        "            'train': train_augmentation,\n",
        "            'val': val_augmentation\n",
        "        }"
      ],
      "metadata": {
        "id": "M6oxBUReOecS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}