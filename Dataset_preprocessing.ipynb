{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahiKhan5360/Segmentation-using-Capsule-layers-and-CNN/blob/main/Dataset_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if running in Colab\n",
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    print(\"Running in Google Colab environment.\")\n",
        "else:\n",
        "    print(\"Not running in Google Colab environment.\")\n",
        "\n",
        "# Mount Google Drive if in Colab\n",
        "if is_colab():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "\n",
        "# Fix for keras.utils.generic_utils error\n",
        "import os\n",
        "os.environ['TF_KERAS'] = '1'  # Force using tf.keras instead of standalone keras\n",
        "\n",
        "# Install required packages\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Fix for TensorFlow Addons compatibility\n",
        "!pip uninstall -y tensorflow-addons\n",
        "!pip install tensorflow-addons==0.17.1\n",
        "\n",
        "# Monkey patch for missing generic_utils\n",
        "import keras\n",
        "if not hasattr(keras.utils, 'generic_utils'):\n",
        "    import tensorflow as tf\n",
        "    keras.utils.generic_utils = tf.keras.utils\n",
        "\n",
        "# Check if packages are already installed to avoid reinstallation\n",
        "required_packages = {\n",
        "    'tensorflow': 'tensorflow>=2.8.0',\n",
        "    'tensorflow_addons': 'tensorflow-addons==0.17.1',  # with specific version\n",
        "    'segmentation_models': 'git+https://github.com/qubvel/segmentation_models',  # Use GitHub version\n",
        "    'albumentations': 'albumentations>=1.1.0',\n",
        "    'opencv-python': 'opencv-python>=4.5.5',\n",
        "    'scikit-image': 'scikit-image>=0.19.2',\n",
        "    'matplotlib': 'matplotlib>=3.5.1',\n",
        "    'pandas': 'pandas>=1.4.2',\n",
        "    'tqdm': 'tqdm>=4.64.0',\n",
        "    'einops': 'einops>=0.4.1',\n",
        "    'timm': 'timm>=0.5.4',\n",
        "    'seaborn': 'seaborn>=0.11.2'\n",
        "}\n",
        "\n",
        "# Only install packages that are not already installed\n",
        "for package, install_name in required_packages.items():\n",
        "    if package == 'tensorflow_addons':  # Skip as we already installed it\n",
        "        continue\n",
        "    if package == 'segmentation_models':  # Special handling for segmentation_models\n",
        "        try:\n",
        "            __import__(package)\n",
        "            print(f\"{package} is already installed.\")\n",
        "        except ImportError:\n",
        "            print(f\"Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", install_name])\n",
        "            print(f\"{package} installed successfully.\")\n",
        "    else:\n",
        "        try:\n",
        "            __import__(package)\n",
        "            print(f\"{package} is already installed.\")\n",
        "        except ImportError:\n",
        "            print(f\"Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", install_name])\n",
        "            print(f\"{package} installed successfully.\")\n",
        "\n",
        "# Set up TensorFlow and GPU configurations\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check for GPU availability\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Set memory growth to avoid allocating all memory at once\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"Found {len(gpus)} GPU(s). Memory growth enabled.\")\n",
        "\n",
        "        # Print GPU information\n",
        "        gpu_info = !nvidia-smi\n",
        "        print(\"GPU Information:\")\n",
        "        for line in gpu_info:\n",
        "            print(line)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"GPU configuration error: {e}\")\n",
        "else:\n",
        "    print(\"No GPU found. Running on CPU.\")\n",
        "\n",
        "# Set up mixed precision for faster training\n",
        "try:\n",
        "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "    tf.keras.mixed_precision.set_global_policy(policy)\n",
        "    print(f\"Mixed precision policy set to: {policy.name}\")\n",
        "except:\n",
        "    print(\"Mixed precision not supported or failed to set up.\")\n",
        "\n",
        "# Print TensorFlow version\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Verify dataset paths\n",
        "import os\n",
        "\n",
        "# Original dataset paths\n",
        "original_input_path = '/content/drive/MyDrive/ISIC2018_original/ISIC2018_Task1_2_Training_Input'\n",
        "original_gt_path = '/content/drive/MyDrive/ISIC2018_original/ISIC2018_Task1_Training_GroundTruth'\n",
        "\n",
        "# Processed dataset paths\n",
        "processed_base_path = '/content/drive/MyDrive/ISIC2018'\n",
        "\n",
        "# Check if original dataset exists\n",
        "if os.path.exists(original_input_path) and os.path.exists(original_gt_path):\n",
        "    input_files = len(os.listdir(original_input_path))\n",
        "    gt_files = len(os.listdir(original_gt_path))\n",
        "    print(f\"Original dataset found:\")\n",
        "    print(f\"  - Input images: {input_files}\")\n",
        "    print(f\"  - Ground truth masks: {gt_files}\")\n",
        "else:\n",
        "    print(\"Warning: Original dataset not found at the specified path.\")\n",
        "    print(f\"Expected paths: \\n  {original_input_path}\\n  {original_gt_path}\")\n",
        "\n",
        "# Check if processed dataset directory exists\n",
        "if not os.path.exists(processed_base_path):\n",
        "    print(f\"Creating processed dataset directory at: {processed_base_path}\")\n",
        "    os.makedirs(processed_base_path, exist_ok=True)\n",
        "else:\n",
        "    print(f\"Processed dataset directory exists at: {processed_base_path}\")\n"
      ],
      "metadata": {
        "id": "fkjZXbsHN1Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import albumentations as A\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "from glob import glob\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define paths (these should match what you set in the setup_environment cell)\n",
        "original_input_path = '/content/drive/MyDrive/ISIC2018_original/ISIC2018_Task1_2_Training_Input'\n",
        "original_gt_path = '/content/drive/MyDrive/ISIC2018_original/ISIC2018_Task1_Training_GroundTruth'\n",
        "processed_base_path = '/content/drive/MyDrive/ISIC2018'\n",
        "\n",
        "class ISIC2018DataPreprocessor:\n",
        "    def __init__(self,\n",
        "                 original_input_path,\n",
        "                 original_gt_path,\n",
        "                 processed_base_path,\n",
        "                 img_size=(256, 256),\n",
        "                 val_split=0.15,\n",
        "                 test_split=0.15):\n",
        "        \"\"\"\n",
        "        Initialize the ISIC2018 data preprocessor.\n",
        "\n",
        "        Args:\n",
        "            original_input_path: Path to original input images\n",
        "            original_gt_path: Path to original ground truth masks\n",
        "            processed_base_path: Base path for processed dataset\n",
        "            img_size: Target image size (height, width)\n",
        "            val_split: Validation split ratio\n",
        "            test_split: Test split ratio\n",
        "        \"\"\"\n",
        "        self.original_input_path = original_input_path\n",
        "        self.original_gt_path = original_gt_path\n",
        "        self.processed_base_path = processed_base_path\n",
        "        self.img_size = img_size\n",
        "        self.val_split = val_split\n",
        "        self.test_split = test_split\n",
        "\n",
        "        # Create processed dataset directories\n",
        "        self.train_img_dir = os.path.join(processed_base_path, 'train', 'images')\n",
        "        self.train_mask_dir = os.path.join(processed_base_path, 'train', 'masks')\n",
        "        self.val_img_dir = os.path.join(processed_base_path, 'validation', 'images')\n",
        "        self.val_mask_dir = os.path.join(processed_base_path, 'validation', 'masks')\n",
        "        self.test_img_dir = os.path.join(processed_base_path, 'test', 'images')\n",
        "        self.test_mask_dir = os.path.join(processed_base_path, 'test', 'masks')\n",
        "\n",
        "        self._create_directories()\n",
        "\n",
        "     def _create_directories(self):\n",
        "        \"\"\"Create necessary directories for processed dataset.\"\"\"\n",
        "        directories = [\n",
        "            self.train_img_dir, self.train_mask_dir,\n",
        "            self.val_img_dir, self.val_mask_dir,\n",
        "            self.test_img_dir, self.test_mask_dir\n",
        "        ]\n",
        "\n",
        "        for directory in directories:\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "        print(\"Created processed dataset directories.\")\n",
        "\n",
        "\n",
        "    def organize_dataset(self):\n",
        "        \"\"\"\n",
        "        Organize the dataset by matching input images with their corresponding masks\n",
        "        and splitting into train, validation, and test sets.\n",
        "        \"\"\"\n",
        "        print(\"Organizing dataset...\")\n",
        "\n",
        "        # Get list of all input images and masks\n",
        "        input_files = sorted(os.listdir(self.original_input_path))\n",
        "        mask_files = sorted(os.listdir(self.original_gt_path))\n",
        "\n",
        "        # Extract image IDs from filenames\n",
        "        input_ids = [os.path.splitext(f)[0] for f in input_files]\n",
        "        mask_ids = [os.path.splitext(f)[0].replace('_segmentation', '') for f in mask_files]\n",
        "\n",
        "        # Create mapping from image ID to filenames\n",
        "        input_map = {id: f for id, f in zip(input_ids, input_files)}\n",
        "        mask_map = {id: f for id, f in zip(mask_ids, mask_files)}\n",
        "\n",
        "        # Find common IDs (images that have corresponding masks)\n",
        "        common_ids = sorted(list(set(input_ids).intersection(set(mask_ids))))\n",
        "\n",
        "        print(f\"Found {len(common_ids)} images with corresponding masks.\")\n",
        "\n",
        "        # Split into train, validation, and test sets\n",
        "        train_ids, temp_ids = train_test_split(\n",
        "            common_ids,\n",
        "            test_size=self.val_split + self.test_split,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Further split temp_ids into validation and test\n",
        "        val_ratio = self.val_split / (self.val_split + self.test_split)\n",
        "        val_ids, test_ids = train_test_split(\n",
        "            temp_ids,\n",
        "            test_size=1 - val_ratio,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        print(f\"Split dataset: {len(train_ids)} train, {len(val_ids)} validation, {len(test_ids)} test\")\n",
        "\n",
        "        # Copy files to their respective directories\n",
        "        self._copy_files(train_ids, input_map, mask_map, self.train_img_dir, self.train_mask_dir)\n",
        "        self._copy_files(val_ids, input_map, mask_map, self.val_img_dir, self.val_mask_dir)\n",
        "        self._copy_files(test_ids, input_map, mask_map, self.test_img_dir, self.test_mask_dir)\n",
        "\n",
        "        return {\n",
        "            'train_ids': train_ids,\n",
        "            'val_ids': val_ids,\n",
        "            'test_ids': test_ids\n",
        "        }\n",
        "\n",
        "\n",
        "    def _copy_files(self, ids, input_map, mask_map, img_dir, mask_dir):\n",
        "        \"\"\"\n",
        "        Copy image and mask files to their respective directories.\n",
        "\n",
        "        Args:\n",
        "            ids: List of image IDs to copy\n",
        "            input_map: Mapping from image ID to input filename\n",
        "            mask_map: Mapping from image ID to mask filename\n",
        "            img_dir: Destination directory for images\n",
        "            mask_dir: Destination directory for masks\n",
        "        \"\"\"\n",
        "        for id in tqdm(ids, desc=f\"Copying files to {os.path.basename(os.path.dirname(img_dir))}\"):\n",
        "            # Copy input image\n",
        "            src_img = os.path.join(self.original_input_path, input_map[id])\n",
        "            dst_img = os.path.join(img_dir, f\"{id}.jpg\")\n",
        "\n",
        "            # Copy mask\n",
        "            src_mask = os.path.join(self.original_gt_path, mask_map[id])\n",
        "            dst_mask = os.path.join(mask_dir, f\"{id}_mask.png\")\n",
        "\n",
        "            # Read, resize, and save image\n",
        "            img = cv2.imread(src_img)\n",
        "            img = cv2.resize(img, self.img_size[::-1])  # cv2 uses (width, height)\n",
        "            cv2.imwrite(dst_img, img)\n",
        "\n",
        "            # Read, resize, and save mask\n",
        "            mask = cv2.imread(src_mask, cv2.IMREAD_GRAYSCALE)\n",
        "            mask = cv2.resize(mask, self.img_size[::-1], interpolation=cv2.INTER_NEAREST)\n",
        "            # Ensure binary mask (0 or 255)\n",
        "            mask = (mask > 127).astype(np.uint8) * 255\n",
        "            cv2.imwrite(dst_mask, mask)\n",
        "\n",
        "    def create_augmentation_pipeline(self):\n",
        "        \"\"\"\n",
        "        Create data augmentation pipelines for training and validation.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing train and validation augmentation pipelines\n",
        "        \"\"\"\n",
        "        # Training augmentations\n",
        "        train_augmentation = A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            A.RandomRotate90(p=0.5),\n",
        "            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "            A.OneOf([\n",
        "                A.ElasticTransform(alpha=120, sigma=120 * 0.05, p=0.5),  # Remove alpha_affine\n",
        "                A.GridDistortion(p=0.5),\n",
        "                A.OpticalDistortion(distort_limit=1.0, p=0.5),  # Remove shift_limit\n",
        "            ], p=0.3),\n",
        "            A.OneOf([\n",
        "                A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=0.5),\n",
        "                A.CLAHE(clip_limit=2.0, p=0.5),\n",
        "                A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
        "            ], p=0.3),\n",
        "\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ])\n",
        "\n",
        "        # Validation/Test augmentations (only normalization)\n",
        "        val_augmentation = A.Compose([\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ])\n",
        "\n",
        "        return {\n",
        "            'train': train_augmentation,\n",
        "            'val': val_augmentation\n",
        "        }\n",
        "\n",
        "    def create_tf_datasets(self, batch_size=16, buffer_size=1000):\n",
        "         # Create augmentation pipelines\n",
        "        train_augmentation = self.create_augmentation_pipeline()['train']\n",
        "        val_augmentation = self.create_augmentation_pipeline()['val']\n",
        "\n",
        "        # Get list of image and mask files\n",
        "        train_img_files = sorted(glob(os.path.join(self.train_img_dir, '*.jpg')))\n",
        "        train_mask_files = sorted(glob(os.path.join(self.train_mask_dir, '*_mask.png')))\n",
        "\n",
        "        val_img_files = sorted(glob(os.path.join(self.val_img_dir, '*.jpg')))\n",
        "        val_mask_files = sorted(glob(os.path.join(self.val_mask_dir, '*_mask.png')))\n",
        "\n",
        "        test_img_files = sorted(glob(os.path.join(self.test_img_dir, '*.jpg')))\n",
        "        test_mask_files = sorted(glob(os.path.join(self.test_mask_dir, '*_mask.png')))\n",
        "\n",
        "        # Define preprocessing function that doesn't capture the augmentation object\n",
        "        def preprocess_train(img_path, mask_path):\n",
        "            # Convert tensors to numpy strings\n",
        "            img_path_str = img_path.numpy().decode('utf-8')\n",
        "            mask_path_str = mask_path.numpy().decode('utf-8')\n",
        "\n",
        "            # Read image and mask\n",
        "            img = cv2.imread(img_path_str)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            mask = cv2.imread(mask_path_str, cv2.IMREAD_GRAYSCALE)\n",
        "            mask = np.expand_dims(mask, axis=-1)\n",
        "\n",
        "            # Apply augmentations\n",
        "            augmented = train_augmentation(image=img, mask=mask)\n",
        "            img_aug = augmented['image']\n",
        "            mask_aug = augmented['mask']\n",
        "\n",
        "            # Normalize mask to [0, 1]\n",
        "            mask_aug = mask_aug / 255.0\n",
        "\n",
        "            return img_aug.astype(np.float32), mask_aug.astype(np.float32)\n",
        "\n",
        "            # Wrap preprocessing functions with tf.py_function\n",
        "        def tf_preprocess_train(img_path, mask_path):\n",
        "            [img, mask] = tf.py_function(\n",
        "                preprocess_train,\n",
        "                [img_path, mask_path],\n",
        "                [tf.float32, tf.float32]\n",
        "            )\n",
        "            img.set_shape([self.img_size[0], self.img_size[1], 3])\n",
        "            mask.set_shape([self.img_size[0], self.img_size[1], 1])\n",
        "            return img, mask\n",
        "\n",
        "        def tf_preprocess_val(img_path, mask_path):\n",
        "            [img, mask] = tf.py_function(\n",
        "                preprocess_val,\n",
        "                [img_path, mask_path],\n",
        "                [tf.float32, tf.float32]\n",
        "            )\n",
        "            img.set_shape([self.img_size[0], self.img_size[1], 3])\n",
        "            mask.set_shape([self.img_size[0], self.img_size[1], 1])\n",
        "            return img, mask\n",
        "        # Create datasets\n",
        "        train_ds = tf.data.Dataset.from_tensor_slices((train_img_files, train_mask_files))\n",
        "        train_ds = train_ds.map(tf_preprocess_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        train_ds = train_ds.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        val_ds = tf.data.Dataset.from_tensor_slices((val_img_files, val_mask_files))\n",
        "        val_ds = val_ds.map(tf_preprocess_val, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        val_ds = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        test_ds = tf.data.Dataset.from_tensor_slices((test_img_files, test_mask_files))\n",
        "        test_ds = test_ds.map(tf_preprocess_val, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        return {\n",
        "            'train': train_ds,\n",
        "            'val': val_ds,\n",
        "            'test': test_ds\n",
        "        }\n",
        "    def visualize_samples(self, num_samples=5):\n",
        "        \"\"\"\n",
        "        Visualize random samples from the training set with augmentations.\n",
        "\n",
        "        Args:\n",
        "            num_samples: Number of samples to visualize\n",
        "        \"\"\"\n",
        "        # Get list of image and mask files\n",
        "        img_files = sorted(glob(os.path.join(self.train_img_dir, '*.jpg')))\n",
        "        mask_files = sorted(glob(os.path.join(self.train_mask_dir, '*_mask.png')))\n",
        "\n",
        "        # Create augmentation pipeline\n",
        "        augmentations = self.create_augmentation_pipeline()\n",
        "\n",
        "        # Select random samples\n",
        "        indices = np.random.choice(len(img_files), num_samples, replace=False)\n",
        "\n",
        "        plt.figure(figsize=(15, 5 * num_samples))\n"
      ],
      "metadata": {
        "id": "M6oxBUReOecS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}